# -*- coding: utf-8 -*-
"""Stroke_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQ9Wa_yi9Nj3yJOlnKtKo3-FAZqS7gGh

## Importing libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt

"""## Importing dataset"""

dataset = pd.read_csv('dataset.csv')

dataset.head()

"""Printing a basic summary of dataset given to us"""

dataset.describe()

"""## Checking empty values

Checking if there are any empty values in the entire dataset
"""

dataset.isnull().sum()

"""## Replacing empty values with mean"""

dataset.bmi.replace(to_replace=np.nan, value=dataset.bmi.mean(), inplace=True)

dataset.isnull().sum()

"""## Data pre-processing

Splitting dataset into dependent and independent variables
"""

x = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values

"""## Encoding categorical data

We are using OneHotEncoder() to encode the categorical columns: 'gender', 'work_type' and 'smoking_status'.
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,5,9])], remainder= 'passthrough')
x = np.array(ct.fit_transform(x))

print(x)

"""We are using LabelEncoder() to encode binary columns: 'ever_married' and 'residence_type'"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
x[:, 15] = le.fit_transform(x[:, 15])
x[:, 16] = le.fit_transform(x[:, 16])

print(x)

"""## Splitting dataset into training and test set"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)

"""Checking dimensions of test set and training set:"""

print("Number transactions x_train dataset: ", x_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions x_test dataset: ", x_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

"""## Feature scaling

StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler results in a distribution with a standard deviation equal to 1.
"""

from sklearn.preprocessing import StandardScaler 
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""## Handling imbalance data using SMOTE

SMOTE - Synthetic Minority Oversampling Technique is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling.
"""

from imblearn.over_sampling import SMOTE

print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))

sm = SMOTE(random_state=2)
x_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0)))

"""## Applying logistic regression on our dataset

Importing necessary libraries for Logistic regression and evaluating metrics
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
from sklearn.model_selection import cross_val_score

"""Applying logistic regression to our model"""

classifier = LogisticRegression(random_state = 0)
classifier.fit(x_train_res, y_train_res)

"""Calculating metrics for our test set"""

y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
accuracies = cross_val_score(estimator = LogisticRegression(random_state=0), X = x_train_res, y = y_train_res, cv = 10)   #K-Fold Validation
roc = roc_auc_score(y_test, y_pred)  #ROC AUC Score
precision = precision_score(y_test, y_pred)  #Precision Score
recall = recall_score(y_test, y_pred)  #Recall Score
f1 = f1_score(y_test, y_pred)  #F1 Score

"""Printing out metrics"""

# Printing metrics
print('Logistic Regression metrics:')
print('')
print('Confusion Matrix:\n',cm)
print('')
print('Accuracy Score: ',accuracy_score(y_test, y_pred))
print('')
print("K-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
print('')
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))
print('')
print('ROC AUC Score: {:.2f}'.format(roc))
print('')
print('Precision: {:.2f}'.format(precision))
print('')
print('Recall: {:.2f}'.format(recall))
print('')
print('F1: {:.2f}'.format(f1))
print('-----------------------------------')
print('')
#Visualizing Confusion Matrix
plt.figure(figsize = (8, 5))
sns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, 
            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])
plt.yticks(rotation = 0)
plt.show()

"""## Advanced Tuning Logistic Regression"""

from sklearn.model_selection import GridSearchCV

"""The GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters."""

j=[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]
grid=GridSearchCV(estimator=LogisticRegression(),param_grid=j,scoring='accuracy',cv=10)
grid.fit(x_train_res, y_train_res)
best_accuracy = grid.best_score_
best_param = grid.best_params_
print('{}:\nBest Accuracy : {:.2f}%'.format('Logistic Regression',best_accuracy*100))
print('Best Parameters : ',best_param)

"""## Interesting Insights and visualizations from dataset

### Some key observations from our dataset

Gender
"""

print(dataset.gender.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="gender")
plt.show()

"""Age & BMI"""

fig=plt.figure(figsize=(20,8),facecolor='white')
gs=fig.add_gridspec(1,2)
ax=[None for i in range(2)]
ax[0]=fig.add_subplot(gs[0,0])
ax[1]=fig.add_subplot(gs[0,1])

ax[0].text(-24,0.019,'Distribution of the age variable',fontsize=23,fontweight='bold', fontfamily='monospace')
ax[0].text(-24,0.01826,'Most of the people in the dataset are between 40 to 60 years old',fontsize=17,fontweight='light', fontfamily='monospace')

ax[1].text(6,412,'Distribution of the bmi variable',fontsize=23,fontweight='bold', fontfamily='monospace')
ax[1].text(6,395,'Most of the people in the dataset are between 25 to 35 of bmi',fontsize=17,fontweight='light', fontfamily='monospace')

sns.kdeplot(x=dataset['age'],ax=ax[0],shade=True, color='gold', alpha=0.6,zorder=3,linewidth=5,edgecolor='black')
sns.histplot(x=dataset['bmi'],ax=ax[1], color='olive', alpha=1,zorder=2,linewidth=1,edgecolor='black',shrink=0.5)

for i in range(2):
    ax[i].set_ylabel('')
    ax[i].grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7))
    
    for direction in ['top','right','left']:
        ax[i].spines[direction].set_visible(False)
        
ax[1].set_xlim(10,70)
        
plt.tight_layout()

"""Hypertension"""

print(dataset.hypertension.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="hypertension")
plt.show()

"""Marital status"""

print(dataset.ever_married.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="ever_married")
plt.show()

"""Type of work """

print(dataset.work_type.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="work_type")
plt.show()

"""Type of residence"""

print(dataset.Residence_type.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="Residence_type")
plt.show()

"""Smoking Status"""

print(dataset.smoking_status.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="smoking_status")
ax.set_xticklabels(ax.get_xticklabels(), fontsize=10)
plt.tight_layout()
plt.show()

"""Stroke experienced?"""

print(dataset.stroke.value_counts())
sns.set_theme(style="darkgrid")
ax = sns.countplot(data=dataset, x="stroke")
plt.show()